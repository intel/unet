{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the medical decathlon dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the tutorial, you will convert the Medical Decathlon raw Nifti files into a single HDF5 file for easier use in TensorFlow/Keras.\n",
    "\n",
    "To begin, you will to download the raw dataset from the Medical Decathlon website (http://medicaldecathlon.com), extract the data (untar), and follow the instructions in this notebook.\n",
    "\n",
    "You may wish to use the code snippet below to easily download the dataset (uncomment to execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir ../../data\n",
    "cd ../../data\n",
    "wget https://raw.githubusercontent.com/pavanjadhaw/gdown.pl/master/gdown.pl && chmod u+x gdown.pl\n",
    "./gdown.pl https://drive.google.com/file/d/1A2IU8Sgea1h3fYLpYtFb2v7NYdMjvEhU/view?usp=sharing Task01_BrainTumour.tar\n",
    "mkdir decathlon\n",
    "tar -xf Task01_BrainTumour.tar --directory ./decathlon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw dataset has the CC-BY-SA 4.0 license.\n",
    "https://creativecommons.org/licenses/by-sa/4.0/\n",
    "\n",
    "For BraTS (Task 1):\n",
    "```\n",
    "INPUT CHANNELS:  \"modality\": {\n",
    "\t \"0\": \"FLAIR\",\n",
    "\t \"1\": \"T1w\",\n",
    "\t \"2\": \"t1gd\",\n",
    "\t \"3\": \"T2w\"\n",
    " },\n",
    "LABEL_CHANNELS: \"labels\": {\n",
    "\t \"0\": \"background\",\n",
    "\t \"1\": \"edema\",\n",
    "\t \"2\": \"non-enhancing tumor\",\n",
    "\t \"3\": \"enhancing tumour\"\n",
    " }\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib  # pip install nibabel\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # pip install tqdm\n",
    "import h5py   # pip install h5py\n",
    "import json\n",
    "\n",
    "import sys; sys.argv=['']; del sys #Make argsparser work in Jupyter notebooks\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Convert Decathlon raw Nifti data (http://medicaldecathlon.com) files to Numpy data files\",\n",
    "    add_help=True, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "parser.add_argument(\"--data_path\",default=\"../../data/decathlon/Task01_BrainTumour/\",help=\"Path to the raw BraTS datafiles\")\n",
    "parser.add_argument(\"--save_path\",default=\"../../data/decathlon/\",help=\"Folder to save Numpy data files\")\n",
    "parser.add_argument(\"--output_filename\",default=\"Task01_BrainTumour.h5\",help=\"Name of the output HDF5 file\")\n",
    "parser.add_argument(\"--resize\", type=int, default=144, help=\"Resize height and width to this size. Original size = 240\")\n",
    "parser.add_argument(\"--split\", type=float, default=0.85, help=\"Train/test split ratio\")\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some functions to perform some image pre-processing such as cropping the center, normalizing the image amongst others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, cropx, cropy, cropz):\n",
    "    \"\"\"\n",
    "    Take a center crop of the images.\n",
    "    If we are using a 2D model, then we'll just stack the\n",
    "    z dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y, z, c = img.shape\n",
    "\n",
    "    # Make sure starting index is >= 0\n",
    "    startx = max(x // 2 - (cropx // 2), 0)\n",
    "    starty = max(y // 2 - (cropy // 2), 0)\n",
    "    startz = max(z // 2 - (cropz // 2), 0)\n",
    "\n",
    "    # Make sure ending index is <= size\n",
    "    endx = min(startx + cropx, x)\n",
    "    endy = min(starty + cropy, y)\n",
    "    endz = min(startz + cropz, z)\n",
    "\n",
    "    return img[startx:endx, starty:endy, startz:endz, :]\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    \"\"\"\n",
    "    Normalize the pixel values.\n",
    "    This is one of the most important preprocessing steps.\n",
    "    We need to make sure that the pixel values have a mean of 0\n",
    "    and a standard deviation of 1 to help the model to train\n",
    "    faster and more accurately.\n",
    "    \"\"\"\n",
    "\n",
    "    for channel in range(img.shape[3]):\n",
    "        img[:, :, :, channel] = (\n",
    "            img[:, :, :, channel] - np.mean(img[:, :, :, channel])) \\\n",
    "            / np.std(img[:, :, :, channel])\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def attach_attributes(df, json_data, name):\n",
    "    \"\"\"\n",
    "    Save the json data\n",
    "    \"\"\"\n",
    "\n",
    "    if type(json_data) is str:\n",
    "        length = 1\n",
    "    else:\n",
    "        length = len(json_data)\n",
    "\n",
    "    dt = h5py.special_dtype(vlen=str)\n",
    "    dset = df.create_dataset(name, (length,), dtype=dt)\n",
    "    dset[:] = json_data\n",
    "\n",
    "\n",
    "def preprocess_inputs(img):\n",
    "    \"\"\"\n",
    "    Process the input images\n",
    "\n",
    "    For BraTS subset:\n",
    "    INPUT CHANNELS:  \"modality\": {\n",
    "         \"0\": \"FLAIR\", T2-weighted-Fluid-Attenuated Inversion Recovery MRI\n",
    "         \"1\": \"T1w\",  T1-weighted MRI\n",
    "         \"2\": \"t1gd\", T1-gadolinium contrast MRI\n",
    "         \"3\": \"T2w\"   T2-weighted MRI\n",
    "     }\n",
    "    \"\"\"\n",
    "    if len(img.shape) != 4:  # Make sure 4D\n",
    "        img = np.expand_dims(img, -1)\n",
    "\n",
    "    img = crop_center(img, args.resize, args.resize, args.resize)\n",
    "    img = normalize_img(img)\n",
    "\n",
    "    img = np.swapaxes(np.array(img), 0, -2)\n",
    "\n",
    "    # img = img[:,:,:,[0]]  # Just get the FLAIR channel\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_labels(msk):\n",
    "    \"\"\"\n",
    "    Process the ground truth labels\n",
    "\n",
    "    For BraTS subset:\n",
    "    LABEL_CHANNELS: \"labels\": {\n",
    "         \"0\": \"background\",  No tumor\n",
    "         \"1\": \"edema\",       Swelling around tumor\n",
    "         \"2\": \"non-enhancing tumor\",  Tumor that isn't enhanced by Gadolinium contrast\n",
    "         \"3\": \"enhancing tumour\"  Gadolinium contrast enhanced regions\n",
    "     }\n",
    "\n",
    "    \"\"\"\n",
    "    if len(msk.shape) != 4:  # Make sure 4D\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "\n",
    "    msk = crop_center(msk, args.resize, args.resize, args.resize)\n",
    "\n",
    "    # Combining all masks assumes that a mask value of 0 is the background\n",
    "    msk[msk > 1] = 1  # Combine all masks\n",
    "    msk = np.swapaxes(np.array(msk), 0, -2)\n",
    "\n",
    "    return msk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet below goes through the Decathlon `dataset.json` file. We've already split into training and validation subsets.\n",
    "The code will read in Nifti (`*.nii`) format files, crop the images and masks. Finally, it saves the result in an HDF5 format.\n",
    "\n",
    "This code is will convert the 3D images and masks into a stack of 2D slices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_raw_data_to_hdf5(trainIdx, validateIdx, testIdx, fileIdx,filename, dataDir, json_data):\n",
    "\n",
    "    hdf_file = h5py.File(filename, \"w\")\n",
    "\n",
    "    # Save the dataset attributes\n",
    "    attach_attributes(hdf_file, str(json_data[\"modality\"]), \"modalities\")\n",
    "    attach_attributes(hdf_file, json_data[\"licence\"], \"license\")\n",
    "    attach_attributes(hdf_file, json_data[\"reference\"], \"reference\")\n",
    "    attach_attributes(hdf_file, json_data[\"name\"], \"name\")\n",
    "    attach_attributes(hdf_file, json_data[\"description\"], \"description\")\n",
    "    attach_attributes(hdf_file, json_data[\"release\"], \"release\")\n",
    "    attach_attributes(\n",
    "        hdf_file, json_data[\"tensorImageSize\"], \"tensorImageSize\")\n",
    "\n",
    "    # Training filenames\n",
    "    train_image_files = []\n",
    "    train_label_files = []\n",
    "    for idx in trainIdx:\n",
    "        train_image_files.append(fileIdx[idx][\"image\"])\n",
    "        train_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Validation filenames\n",
    "    validate_image_files = []\n",
    "    validate_label_files = []\n",
    "    for idx in validateIdx:\n",
    "        validate_image_files.append(fileIdx[idx][\"image\"])\n",
    "        validate_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    # Testing filenames\n",
    "    test_image_files = []\n",
    "    test_label_files = []\n",
    "    for idx in testIdx:\n",
    "        test_image_files.append(fileIdx[idx][\"image\"])\n",
    "        test_label_files.append(fileIdx[idx][\"label\"])\n",
    "\n",
    "    attach_attributes(hdf_file, train_image_files, \"training_input_files\")\n",
    "    attach_attributes(hdf_file, train_label_files, \"training_label_files\")\n",
    "    attach_attributes(hdf_file, validate_image_files, \"validation_input_files\")\n",
    "    attach_attributes(hdf_file, validate_label_files, \"validation_label_files\")\n",
    "    attach_attributes(hdf_file, test_image_files, \"testing_input_files\")\n",
    "    attach_attributes(hdf_file, test_label_files, \"testing_label_files\")\n",
    "\n",
    "    \"\"\"\n",
    "    Print shapes of raw data\n",
    "    \"\"\"\n",
    "    print(\"Data shapes\")\n",
    "    print(\"===========\")\n",
    "    print(\"n.b. All tensors converted to stacks of 2D slices.\")\n",
    "    print(\"If you want true 3D tensors, then modify this code appropriately.\")\n",
    "    data_filename = os.path.join(dataDir, train_image_files[0])\n",
    "    img = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Image shape     = \", img.shape)\n",
    "    crop_shape = preprocess_inputs(img).shape[1:]\n",
    "    print(\"Cropped Image shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    data_filename = os.path.join(dataDir, train_label_files[0])\n",
    "    msk = np.array(nib.load(data_filename).dataobj)\n",
    "    print(\"Raw Masks shape     = \", msk.shape)\n",
    "    crop_shape = preprocess_labels(msk).shape[1:]\n",
    "    print(\"Cropped Masks shape = (?, {}, {}, {})\".format(crop_shape[0],\n",
    "                                                         crop_shape[1],\n",
    "                                                         crop_shape[2]))\n",
    "\n",
    "    # Save training set images\n",
    "    print(\"Step 1 of 6. Save training set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_image_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "\n",
    "        img = preprocess_inputs(img)\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_train_dset = hdf_file.create_dataset(\"imgs_train\",\n",
    "                                                     img.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               img.shape[1],\n",
    "                                                               img.shape[2],\n",
    "                                                               img.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            img_train_dset[:] = img\n",
    "        else:\n",
    "            row = img_train_dset.shape[0]  # Count current dataset rows\n",
    "            img_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_train_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 2 of 6. Save validation set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_validation_dset = hdf_file.create_dataset(\"imgs_validation\",\n",
    "                                                          img.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    img.shape[1],\n",
    "                                                                    img.shape[2],\n",
    "                                                                    img.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            img_validation_dset[:] = img\n",
    "        else:\n",
    "            row = img_validation_dset.shape[0]  # Count current dataset rows\n",
    "            img_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_validation_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save validation set images\n",
    "    print(\"Step 3 of 6. Save testing set images.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_image_files):\n",
    "\n",
    "        # Nibabel should read the file as X,Y,Z,C\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        img = np.array(nib.load(data_filename).dataobj)\n",
    "        img = preprocess_inputs(img)\n",
    "\n",
    "        num_rows = img.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            img_testing_dset = hdf_file.create_dataset(\"imgs_testing\",\n",
    "                                                       img.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 img.shape[1],\n",
    "                                                                 img.shape[2],\n",
    "                                                                 img.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            img_testing_dset[:] = img\n",
    "        else:\n",
    "            row = img_testing_dset.shape[0]  # Count current dataset rows\n",
    "            img_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            img_testing_dset[row:(row + num_rows), :] = img\n",
    "\n",
    "    # Save training set masks\n",
    "    print(\"Step 4 of 6. Save training set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(train_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_train_dset = hdf_file.create_dataset(\"msks_train\",\n",
    "                                                     msk.shape,\n",
    "                                                     maxshape=(None,\n",
    "                                                               msk.shape[1],\n",
    "                                                               msk.shape[2],\n",
    "                                                               msk.shape[3]),\n",
    "                                                     dtype=float,\n",
    "                                                     compression=\"gzip\")\n",
    "            msk_train_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_train_dset.shape[0]  # Count current dataset rows\n",
    "            msk_train_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_train_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    # Save testing/validation set masks\n",
    "\n",
    "    print(\"Step 5 of 6. Save validation set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(validate_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_validation_dset = hdf_file.create_dataset(\"msks_validation\",\n",
    "                                                          msk.shape,\n",
    "                                                          maxshape=(None,\n",
    "                                                                    msk.shape[1],\n",
    "                                                                    msk.shape[2],\n",
    "                                                                    msk.shape[3]),\n",
    "                                                          dtype=float,\n",
    "                                                          compression=\"gzip\")\n",
    "            msk_validation_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_validation_dset.shape[0]  # Count current dataset rows\n",
    "            msk_validation_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_validation_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    print(\"Step 6 of 6. Save testing set masks.\")\n",
    "    first = True\n",
    "    for idx in tqdm(test_label_files):\n",
    "\n",
    "        data_filename = os.path.join(dataDir, idx)\n",
    "        msk = np.array(nib.load(data_filename).dataobj)\n",
    "        msk = preprocess_labels(msk)\n",
    "\n",
    "        num_rows = msk.shape[0]\n",
    "\n",
    "        if first:\n",
    "            first = False\n",
    "            msk_testing_dset = hdf_file.create_dataset(\"msks_testing\",\n",
    "                                                       msk.shape,\n",
    "                                                       maxshape=(None,\n",
    "                                                                 msk.shape[1],\n",
    "                                                                 msk.shape[2],\n",
    "                                                                 msk.shape[3]),\n",
    "                                                       dtype=float,\n",
    "                                                       compression=\"gzip\")\n",
    "            msk_testing_dset[:] = msk\n",
    "        else:\n",
    "            row = msk_testing_dset.shape[0]  # Count current dataset rows\n",
    "            msk_testing_dset.resize(row + num_rows, axis=0)  # Add new row\n",
    "            # Insert data into new row\n",
    "            msk_testing_dset[row:(row + num_rows), :] = msk\n",
    "\n",
    "    hdf_file.close()\n",
    "    print(\"Finished processing.\")\n",
    "    print(\"HDF5 saved to {}\".format(filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the functions have been defined, let's proceed with converting the Decathlon raw Nifti data files to single training and validation HDF5 data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(\n",
    "args.save_path, \"{}x{}/\".format(args.resize, args.resize))\n",
    "\n",
    "# Create directory\n",
    "try:\n",
    "    os.makedirs(save_dir)\n",
    "except OSError:\n",
    "    if not os.path.isdir(save_dir):\n",
    "        raise\n",
    "\n",
    "filename = os.path.join(save_dir, args.output_filename)\n",
    "# Check for existing output file and delete if exists\n",
    "if os.path.exists(filename):\n",
    "    print(\"Removing existing data file: {}\".format(filename))\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training file names from the data directory. Decathlon should always have a `dataset.json` file in the\n",
    "subdirectory which lists the experiment information including the input and label filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "==============================\n",
      "Dataset name:         BRATS\n",
      "Dataset description:  Gliomas segmentation tumour and oedema in on brain images\n",
      "Tensor image size:    4D\n",
      "Dataset release:      2.0 04/05/2018\n",
      "Dataset reference:    https://www.med.upenn.edu/sbia/brats2017.html\n",
      "Dataset license:      CC-BY-SA 4.0\n",
      "==============================\n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "json_filename = os.path.join(args.data_path, \"dataset.json\")\n",
    "\n",
    "try:\n",
    "    with open(json_filename, \"r\") as fp:\n",
    "        experiment_data = json.load(fp)\n",
    "except IOError as e:\n",
    "    print(\"File {} doesn't exist. It should be part of the \"\" Decathlon directory\".format(json_filename))\n",
    "\n",
    "# Print information about the Decathlon experiment data\n",
    "print(\"*\" * 30)\n",
    "print(\"=\" * 30)\n",
    "print(\"Dataset name:        \", experiment_data[\"name\"])\n",
    "print(\"Dataset description: \", experiment_data[\"description\"])\n",
    "print(\"Tensor image size:   \", experiment_data[\"tensorImageSize\"])\n",
    "print(\"Dataset release:     \", experiment_data[\"release\"])\n",
    "print(\"Dataset reference:   \", experiment_data[\"reference\"])\n",
    "print(\"Dataset license:     \", experiment_data[\"licence\"])  # sic\n",
    "print(\"=\" * 30)\n",
    "print(\"*\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nexty, we will randomize the file list. After that, we will separate into training and validation lists. We won't use the testing set since we don't have ground truth masks for this; instead we'll split the validation set into separate test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes\n",
      "===========\n",
      "n.b. All tensors converted to stacks of 2D slices.\n",
      "If you want true 3D tensors, then modify this code appropriately.\n",
      "Raw Image shape     =  (240, 240, 155, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/406 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped Image shape = (?, 144, 144, 4)\n",
      "Raw Masks shape     =  (240, 240, 155)\n",
      "Cropped Masks shape = (?, 144, 144, 1)\n",
      "Step 1 of 6. Save training set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [18:29<00:00,  2.51s/it]\n",
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 of 6. Save validation set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:27<00:00,  2.81s/it]\n",
      "  0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 of 6. Save testing set images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [02:06<00:00,  2.76s/it]\n",
      "  0%|          | 1/406 [00:00<01:09,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 of 6. Save training set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 406/406 [01:13<00:00,  5.60it/s]\n",
      "  3%|▎         | 1/32 [00:00<00:05,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 of 6. Save validation set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  5.62it/s]\n",
      "  2%|▏         | 1/46 [00:00<00:08,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 of 6. Save testing set masks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:08<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing.\n",
      "HDF5 saved to ../../data/decathlon/144x144/Task01_BrainTumour.h5\n"
     ]
    }
   ],
   "source": [
    "# Set the random seed so that always get same random mix\n",
    "np.random.seed(816)\n",
    "numFiles = experiment_data[\"numTraining\"]\n",
    "idxList = np.arange(numFiles)  # List of file indices\n",
    "randomList = np.random.random(numFiles)  # List of random numbers\n",
    "# Random number go from 0 to 1. So anything above\n",
    "# args.train_split is in the validation list.\n",
    "trainList = idxList[randomList < args.split]\n",
    "\n",
    "otherList = idxList[randomList >= args.split]\n",
    "randomList = np.random.random(len(otherList))  # List of random numbers\n",
    "validateList = otherList[randomList >= 0.5]\n",
    "testList = otherList[randomList < 0.5]\n",
    "\n",
    "convert_raw_data_to_hdf5(trainList, validateList, testList,\n",
    "                         experiment_data[\"training\"],\n",
    "                         filename, args.data_path,\n",
    "                         experiment_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. SPDX-License-Identifier: EPL-2.0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Copyright (c) 2019 Intel Corporation`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
